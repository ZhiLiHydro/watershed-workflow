{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Complete Workflow: Coweeta Hydrologic Laboratory\n",
    "\n",
    "This workflow provides a complete working example to develop an unstructured mesh for an integrated hydrologic model at the Coweeta Hydrologic Laboratory at the Southern Research Station of the US Forrest Service.\n",
    "\n",
    "It uses the following datasets:\n",
    "\n",
    "* `NHD Plus` for the watershed boundary and hydrography.\n",
    "* `NED` for elevation\n",
    "* `NLCD` for land cover/transpiration/rooting depths\n",
    "* `GLYHMPS` geology data for structural formations\n",
    "* `SoilGrids 2017` for depth to bedrock\n",
    "* `SSURGO` for soil data, where available, in the top 2m.\n",
    "\n",
    "This is known as the \"default\" workflow because it is expected to work on any catchment in the conterminous US.  Other, more complicated soil structure workflows can be applied, but they are a bit less robust and often require gapfilling or making educated decisions about parameters.  This should need little to no user changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import shapely\n",
    "import logging\n",
    "import scipy.ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas\n",
    "pandas.options.display.max_columns = None\n",
    "\n",
    "import workflow\n",
    "import workflow.source_list\n",
    "import workflow.ui\n",
    "import workflow.colors\n",
    "import workflow.condition\n",
    "import workflow.mesh\n",
    "import workflow.split_hucs\n",
    "import workflow.soil_properties\n",
    "\n",
    "workflow.ui.setup_logging(1,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following will be used to write an ATS input file\n",
    "import ats_input_spec\n",
    "import ats_input_spec.public\n",
    "import ats_input_spec.io\n",
    "import amanzi_xml.utils.io as aio\n",
    "import amanzi_xml.utils.search as asearch\n",
    "import amanzi_xml.utils.errors as aerrors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, by default, we tend to work in the DayMet CRS because this allows us to avoid reprojecting meteorological forcing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the input shapefile and a hint as to what HUC it is in.\n",
    "coweeta_shapefile = '../input_data/coweeta_basin.shp'\n",
    "hint = '0601'  # hint: HUC 4 containing this shape.  \n",
    "               # This is necessary to avoid downloading all HUCs to search for this shape\n",
    "\n",
    "crs = workflow.crs.daymet_crs()\n",
    "output_folder = \"../workflow_output\"\n",
    "\n",
    "def get_filename(myfile):\n",
    "    return os.path.join(output_folder, myfile)\n",
    "\n",
    "figsize = (6,6)\n",
    "figsize_3d = (8,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources and setup\n",
    "\n",
    "Next we set up the source watershed and coordinate system and all data sources for our mesh.  We will use the CRS that is included in the shapefile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wide range of data sources are available; here we use the defaults except for using NHD Plus for watershed boundaries and hydrography (the default is NHD, which is lower resolution and therefore smaller download sizes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-13 11:54:45,065 - root - INFO: Using sources:\n",
      "2021-07-13 11:54:45,067 - root - INFO: --------------\n",
      "2021-07-13 11:54:45,067 - root - INFO: HUC: National Hydrography Dataset Plus High Resolution (NHDPlus HR)\n",
      "2021-07-13 11:54:45,068 - root - INFO: hydrography: National Hydrography Dataset Plus High Resolution (NHDPlus HR)\n",
      "2021-07-13 11:54:45,068 - root - INFO: DEM: National Elevation Dataset (NED)\n",
      "2021-07-13 11:54:45,069 - root - INFO: soil structure: National Resources Conservation Service Soil Survey (NRCS Soils)\n",
      "2021-07-13 11:54:45,069 - root - INFO: geologic structure: GLHYMPS version 2.0\n",
      "2021-07-13 11:54:45,070 - root - INFO: land cover: National Land Cover Database (NLCD) Layer: NLCD_2016_Land_Cover_L48\n",
      "2021-07-13 11:54:45,070 - root - INFO: soil thickness: None\n",
      "2021-07-13 11:54:45,071 - root - INFO: meteorology: DayMet 1km\n"
     ]
    }
   ],
   "source": [
    "# set up a dictionary of source objects\n",
    "sources = workflow.source_list.get_default_sources()\n",
    "sources['hydrography'] = workflow.source_list.hydrography_sources['NHD Plus']\n",
    "sources['HUC'] = workflow.source_list.huc_sources['NHD Plus']\n",
    "workflow.source_list.log_sources(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-13 11:54:45,077 - root - INFO: \n",
      "2021-07-13 11:54:45,078 - root - INFO: Loading shapes\n",
      "2021-07-13 11:54:45,079 - root - INFO: ------------------------------\n",
      "2021-07-13 11:54:45,080 - root - INFO: Loading file: '../input_data/coweeta_basin.shp'\n",
      "2021-07-13 11:54:45,101 - root - INFO: ... found 1 shapes\n",
      "2021-07-13 11:54:45,102 - root - INFO: Converting to shapely\n",
      "2021-07-13 11:54:45,124 - root - INFO: Converting to requested CRS\n"
     ]
    }
   ],
   "source": [
    "# load the shape\n",
    "_, watershed = workflow.get_split_form_shapes(coweeta_shapefile, out_crs=crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the surface mesh\n",
    "\n",
    "First we'll generate the flattened, 2D triangulation, which builds on hydrography data.  Then we download a digital elevation map from the National Elevation Dataset, and extrude that 2D triangulation to a 3D surface mesh based on interpolation between pixels of the DEM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get river network\n",
    "\n",
    "This will download the river network from the NHD Plus database, and simplify the network, constructing a tree-like data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-13 11:54:45,179 - root - INFO: \n",
      "2021-07-13 11:54:45,179 - root - INFO: Loading Hydrography\n",
      "2021-07-13 11:54:45,180 - root - INFO: ------------------------------\n",
      "2021-07-13 11:54:45,180 - root - INFO: Loading streams in HUC 0601\n",
      "2021-07-13 11:54:45,181 - root - INFO:          and/or bounds (1442132.8431974074, -650282.048183706, 1447117.7928619592, -645090.2638397965)\n",
      "2021-07-13 11:54:45,183 - root - INFO:   Using Hydrography file \"/Users/uec/code/watershed_workflow/data-library/hydrography/NHDPlus_H_0601_GDB/NHDPlus_H_0601.gdb\"\n",
      "2021-07-13 11:54:45,184 - root - INFO:   National Hydrography Dataset Plus High Resolution (NHDPlus HR): opening '/Users/uec/code/watershed_workflow/data-library/hydrography/NHDPlus_H_0601_GDB/NHDPlus_H_0601.gdb' layer 'NHDFlowline' for streams in '(1442132.8431974074, -650282.048183706, 1447117.7928619592, -645090.2638397965)'\n",
      "/Users/uec/code/watershed_workflow/repos/master/workflow/sources/manager_nhd.py:171: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  reaches = [r for (i,r) in fid.items(bbox=bounds)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a3cfbda71675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrivers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# download/collect the river network within that shape's bounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     _, reaches = workflow.get_reaches(sources['hydrography'], hint, \n\u001b[0m\u001b[1;32m      7\u001b[0m                                       watershed.exterior().bounds, crs, crs)\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# simplify and prune rivers not IN the shape, constructing a tree-like data structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/watershed_workflow/repos/master/workflow/hilev.py\u001b[0m in \u001b[0;36mget_reaches\u001b[0;34m(source, huc, bounds, in_crs, out_crs, digits, long, merge, presimplify)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;31m# get the reaches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m     \u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hydro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_crs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"... found {} reaches\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreaches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/watershed_workflow/repos/master/workflow/sources/manager_nhd.py\u001b[0m in \u001b[0;36mget_hydro\u001b[0;34m(self, huc, bounds, bounds_crs, in_network, force_download)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds_crs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_fiona\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'crs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mreaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# filter not in network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/watershed_workflow/repos/master/workflow/sources/manager_nhd.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds_crs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_fiona\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'crs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mreaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# filter not in network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.ItemsIterator.__next__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/Shared/ornldev/code/miniconda3/envs/watershed_workflow_20210603/lib/python3.9/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36mdriver\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mDriverError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unsupported mode: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;34m\"\"\"Returns the name of the proper OGR driver.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "simplify = 30 # length scale to target average edge\n",
    "\n",
    "rivers = True\n",
    "if rivers:\n",
    "    # download/collect the river network within that shape's bounds\n",
    "    _, reaches = workflow.get_reaches(sources['hydrography'], hint, \n",
    "                                      watershed.exterior().bounds, crs, crs)\n",
    "    # simplify and prune rivers not IN the shape, constructing a tree-like data structure\n",
    "    # for the river network\n",
    "    rivers = workflow.simplify_and_prune(watershed, reaches, filter=True, simplify=simplify, \n",
    "                                         snap=True, cut_intersections=True)\n",
    "\n",
    "\n",
    "else:\n",
    "    rivers = list()\n",
    "    workflow.split_hucs.simplify(watershed, simplify)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot what we have so far -- an image of the HUC and its stream network\n",
    "fig, ax = workflow.plot.get_ax(crs)\n",
    "\n",
    "workflow.plot.hucs(watershed, crs, ax=ax, color='k', linewidth=1)\n",
    "workflow.plot.rivers(rivers, crs, ax=ax, color='red', linewidth=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate meshes using river network and watershed shape\n",
    "\n",
    "Triangulation refinement: refine triangles if their area (in m^2) is greater than A(d), where d is the \n",
    "distance from the triangle centroid to the nearest stream.  A(d) is a piecewise linear function -- A = A0 if d <= d0, A = A1 if d >= d1, and linearly interpolates between the two endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = 100; d1 = 500\n",
    "A0 = 1000; A1 = 5000\n",
    "\n",
    "# Refine triangles if they get too acute\n",
    "min_angle = 32 # degrees\n",
    "\n",
    "# make 2D mesh\n",
    "mesh_points2, mesh_tris, areas, distances = workflow.triangulate(watershed, rivers, \n",
    "                                               refine_distance=[d0,A0,d1,A1],\n",
    "                                               refine_min_angle=min_angle,\n",
    "                                               diagnostics=True,\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map mesh to DEM\n",
    "\n",
    "Download a DEM from USGS NED and elevate the triangle nodes to the DEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download the needed rasters\n",
    "dem_profile, dem = workflow.get_raster_on_shape(sources['DEM'], watershed.exterior(), crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noting that the DEM is a 30m raster, and we want to run at a coarser resolution of ~100-300m, \n",
    "# the DEM will look quite rough.  Smooth a small amount.  Note better algorithms could be used \n",
    "# here, but for now we just use Gaussian smoothing.\n",
    "dem_sm = scipy.ndimage.gaussian_filter(dem, 3, mode='nearest')\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "axs[0].imshow(dem)\n",
    "axs[0].set_title('unsmoothed')\n",
    "axs[1].imshow(dem_sm)\n",
    "txt = axs[1].set_title('smoothed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elevate the x,y points onto the DEM to get a z coordinate\n",
    "mesh_points3 = workflow.elevate(mesh_points2, crs, dem_sm, dem_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the 2D mesh\n",
    "m2 = workflow.mesh.Mesh2D(mesh_points3.copy(), list(mesh_tris))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydrologically condition the mesh, removing pits\n",
    "workflow.condition.fill_pits(m2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the resulting surface mesh\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = workflow.plot.get_ax(crs, fig, window=[0.05,0.1,0.9,0.8])\n",
    "#ax2 = workflow.plot.get_ax(crs,fig, window=[0.65,0.05,0.3,0.5])\n",
    "ax2 = ax.inset_axes([0.65,0.05,0.3,0.3])\n",
    "cbax = fig.add_axes([0.05,0.05,0.9,0.05])\n",
    "\n",
    "# print(watershed.exterior().bounds)\n",
    "# bounds: 1442135.595488033, -650282.048183706, 1447110.5975158156, -645090.2638397965\n",
    "xlim = (1444800., 1445200.)\n",
    "ylim = (-646700, -646300)\n",
    "\n",
    "mp = workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color='elevation', edgecolor='white', linewidth=0.2)\n",
    "cbar = fig.colorbar(mp, orientation=\"horizontal\", cax=cbax)\n",
    "workflow.plot.hucs(watershed, crs, ax=ax, color='k', linewidth=1)\n",
    "workflow.plot.rivers(rivers, crs, ax=ax, color='red', linewidth=1)\n",
    "ax.set_aspect('equal', 'datalim')\n",
    "\n",
    "mp2 = workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax2, \n",
    "                                 color='elevation', edgecolor='white', linewidth=0.2)\n",
    "workflow.plot.hucs(watershed, crs, ax=ax2, color='k', linewidth=1)\n",
    "workflow.plot.rivers(rivers, crs, ax=ax2, color='red', linewidth=1.5)\n",
    "ax2.set_xlim(xlim)\n",
    "ax2.set_ylim(ylim)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "ax.indicate_inset_zoom(ax2, edgecolor='k')\n",
    "\n",
    "\n",
    "print(ax.get_xlim())\n",
    "print(ax.get_ylim())\n",
    "cbar.ax.set_title('elevation [m]')\n",
    "fig.savefig(get_filename('coweeta_dem'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface properties\n",
    "\n",
    "Meshes interact with data to provide forcing, parameters, and more in the actual simulation.  Specifically, we need vegetation type on the surface to provide information about transpiration and subsurface structure to provide information about water retention curves, etc.\n",
    "\n",
    "We'll start by downloading and collecting land cover from the NLCD dataset, and generate sets for each land cover type that cover the surface.  Likely these will be some combination of grass, deciduous forest, coniferous forest, and mixed forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the NLCD raster\n",
    "lc_profile, lc_raster = workflow.get_raster_on_shape(sources['land cover'], \n",
    "                                                     watershed.exterior(), crs)\n",
    "\n",
    "# resample the raster to the triangles\n",
    "lc = workflow.values_from_raster(m2.centroids(), crs, lc_raster, lc_profile)\n",
    "\n",
    "# what land cover types did we get?\n",
    "logging.info('Found land cover dtypes: {}'.format(lc.dtype))\n",
    "logging.info('Found land cover types: {}'.format(set(lc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the image\n",
    "# -- get the NLCD colormap which uses official NLCD colors and labels\n",
    "nlcd_indices, nlcd_cmap, nlcd_norm, nlcd_ticks, nlcd_labels = \\\n",
    "                workflow.colors.generate_nlcd_colormap(lc)\n",
    "\n",
    "fig, ax = workflow.plot.get_ax(crs, figsize=figsize)\n",
    "polys = workflow.plot.mesh(m2, crs, ax=ax, color=lc, cmap=nlcd_cmap, norm=nlcd_norm, edgecolor='none', \n",
    "                                     facecolor='color', linewidth=0.5)\n",
    "\n",
    "workflow.colors.colorbar_index(ncolors=len(np.unique(lc)), cmap=nlcd_cmap, labels = nlcd_labels) \n",
    "\n",
    "ax.set_title(\"Land Cover Index\")\n",
    "ext = ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't really need all of these.  Keep Evergreen, Deciduous, Shrub, and merge the rest into \"Other\"\n",
    "nlcd_color_new = 99 * np.ones_like(lc)\n",
    "\n",
    "groupings = {\n",
    "    43 : ['Mixed Forest', 'Evergreen Forest',],\n",
    "    41 : ['Deciduous Forest',],\n",
    "    81 : ['Dwarf Scrub', 'Shrub/Scrub', 'Grassland/Herbaceous', 'Sedge/Herbaceous', \n",
    "                     'Pasture/Hay', 'Cultivated Crops'],\n",
    "}\n",
    "\n",
    "for k,v in groupings.items():\n",
    "    for label in v:\n",
    "        index = sources['land cover'].indices[label]\n",
    "        nlcd_color_new[np.where(lc == index)] = k\n",
    "    \n",
    "print(nlcd_color_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the updated image, adding \"other\"\n",
    "nlcd_color_new_other_as_water = np.where(nlcd_color_new == 99, 11, nlcd_color_new)\n",
    "\n",
    "# -- get the NLCD colormap which uses official NLCD colors and labels\n",
    "nlcd_indices, nlcd_cmap, nlcd_norm, nlcd_ticks, nlcd_labels = \\\n",
    "                workflow.colors.generate_nlcd_colormap(nlcd_color_new_other_as_water)\n",
    "\n",
    "# make (water, 11) into (other, 99)\n",
    "nlcd_labels[0] = 'Other'\n",
    "nlcd_indices[0] = 99\n",
    "\n",
    "fig, ax = workflow.plot.get_ax(crs, figsize=figsize)\n",
    "\n",
    "\n",
    "polys = workflow.plot.mesh(m2, crs, ax=ax, color=nlcd_color_new_other_as_water, \n",
    "                           cmap=nlcd_cmap, norm=nlcd_norm, edgecolor='none', \n",
    "                        facecolor='color', linewidth=0.5)\n",
    "\n",
    "workflow.colors.colorbar_index(ncolors=len(np.unique(nlcd_color_new_other_as_water)), \n",
    "                               cmap=nlcd_cmap, labels = nlcd_labels) \n",
    "\n",
    "ax.set_title(\"Land Cover Index\")\n",
    "ext = ax.axis('off')\n",
    "plt.tight_layout()\n",
    "fig.savefig(get_filename('coweeta_landcover'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsurface properties\n",
    "\n",
    "The default model uses GLHYMPS to identify geologic formations, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the NRCS soils data as shapes and project it onto the mesh\n",
    "\n",
    "# -- download the shapes\n",
    "target_bounds = watershed.exterior().bounds\n",
    "logging.info('target bounds: {}'.format(target_bounds))\n",
    "soil_profile, soil_survey, soil_survey_props = workflow.get_shapes(sources['soil structure'], \n",
    "                                                                   target_bounds, crs, crs, properties=True)\n",
    "\n",
    "# -- determine the NRCS mukey for each soil unit; this uniquely identifies soil \n",
    "#    properties\n",
    "soil_ids = np.array([shp.properties['mukey'] for shp in soil_survey], np.int32)\n",
    "\n",
    "# -- color a raster by the polygons (this makes identifying a triangle's value much \n",
    "#    more efficient)\n",
    "soil_color_raster, soil_color_profile, img_bounds = \\\n",
    "            workflow.color_raster_from_shapes(target_bounds, 10, soil_survey,\n",
    "                                              soil_ids, crs)\n",
    "\n",
    "# -- resample the raster to the triangles\n",
    "soil_color = workflow.values_from_raster(m2.centroids(), crs, \n",
    "                                         soil_color_raster, soil_color_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the soils within the watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_survey_props.set_index('mukey', inplace=True, drop=False)\n",
    "soil_survey_props = soil_survey_props.loc[np.unique(soil_color), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the soil mukey\n",
    "indices, cmap, norm, ticks, labels = workflow.colors.generate_indexed_colormap(soil_color, cmap='tab20c')\n",
    "fig, ax = workflow.plot.get_ax(crs, figsize=figsize)\n",
    "\n",
    "mp = workflow.plot.mesh(m2, crs, ax=ax, facecolor='color',\n",
    "                        linewidth=0, color=soil_color, \n",
    "                        cmap=cmap, norm = norm\n",
    "                       )\n",
    "\n",
    "workflow.colors.colorbar_index(ncolors=len(np.unique(soil_color)), cmap=cmap, labels = labels) \n",
    "\n",
    "ax.set_title('soil type index')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "fig.savefig(get_filename('coweeta_soils'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does soil thickness look like?\n",
    "soil_thickness = np.empty(soil_color.shape, 'd')\n",
    "for mukey in soil_survey_props.index:\n",
    "    soil_thickness[soil_color == mukey] = soil_survey_props.loc[mukey,'thickness [cm]']\n",
    "\n",
    "soil_thickness = soil_thickness / 100\n",
    "fig, ax = workflow.plot.get_ax(crs, figsize=figsize)\n",
    "mp = workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color=soil_thickness, edgecolor='gray', cmap='jet')\n",
    "ax.set_title('soil thickness [m]')\n",
    "cb = fig.colorbar(mp, fraction=0.04, pad=0.04)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "\n",
    "print('Median soil thickness [-] = ', np.nanmedian(soil_thickness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of porosity from SSURGO\n",
    "iprop = np.empty(soil_color.shape, 'd')\n",
    "for mukey in soil_survey_props.index:\n",
    "    iprop[soil_color == mukey] = soil_survey_props.loc[ mukey,'porosity [-]']\n",
    "\n",
    "fig, ax = workflow.plot.get_ax(crs, figsize=figsize)\n",
    "mp = workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color=iprop, edgecolor='gray', cmap='jet_r')\n",
    "ax.set_title('soil porosity [-]')\n",
    "cb = fig.colorbar(mp, fraction=0.04, pad=0.04, extend = \"both\", shrink = 0.6)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "fig.savefig(get_filename('coweeta_soil_porosity'))\n",
    "\n",
    "print('Median porosity [-] = ', np.nanmedian(iprop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of permeability\n",
    "iprop = np.empty(soil_color.shape, 'd')\n",
    "for mukey in soil_survey_props.index:\n",
    "    iprop[soil_color == mukey] = soil_survey_props.loc[ mukey,'permeability [m^2]']\n",
    "\n",
    "fig, ax = workflow.plot.get_ax(crs)\n",
    "mp = workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color=np.log10(iprop), edgecolor='gray', cmap='jet')\n",
    "ax.set_title('soil permeability [-]')\n",
    "cb = fig.colorbar(mp, fraction=0.04, pad=0.04, extend = \"both\", shrink = 0.6)\n",
    "cb.ax.set_title('log K')\n",
    "ax.axis('off')\n",
    "fig.savefig(get_filename('coweeta_soil_permeability'))\n",
    "\n",
    "print('Min k [m^2] = ', np.nanmin(iprop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the missing data (white).  This is because some SSURGO map units have no formation with complete \n",
    "# information.  So we merge the above available data, filling where possible and dropping regions that\n",
    "# do not have a complete set of properties.\n",
    "soil_survey_props_clean = soil_survey_props.copy()\n",
    "\n",
    "# later scripts expect 'native_index' as a standard name of holding onto the original IDs\n",
    "soil_survey_props_clean.rename_axis('native_index', inplace=True)\n",
    "soil_survey_props_clean.rename(columns={'mukey':'native_index'}, inplace=True)\n",
    "\n",
    "# need thickness in m\n",
    "soil_survey_props_clean['thickness [cm]'] = soil_survey_props_clean['thickness [cm]']/100.\n",
    "soil_survey_props_clean.rename(columns={'thickness [cm]':'thickness [m]'}, inplace=True)\n",
    "\n",
    "\n",
    "def replace_column_nans(df, col_nan, col_replacement):\n",
    "    \"\"\"In a df, replace col_nan entries by col_replacement if is nan.  In Place!\"\"\"\n",
    "    row_indexer = df[col_nan].isna()\n",
    "    df.loc[row_indexer, col_nan] = df.loc[row_indexer, col_replacement]\n",
    "    return\n",
    "\n",
    "# where poro or perm is nan, put Rosetta poro\n",
    "replace_column_nans(soil_survey_props_clean, 'porosity [-]', 'Rosetta porosity [-]')\n",
    "replace_column_nans(soil_survey_props_clean, 'permeability [m^2]', 'Rosetta permeability [m^2]')\n",
    "\n",
    "# drop unnecessary columns\n",
    "for col in ['Rosetta porosity [-]', 'Rosetta permeability [m^2]', 'bulk density [g/cm^3]', 'total sand pct [%]',\n",
    "            'total silt pct [%]', 'total clay pct [%]']:\n",
    "    soil_survey_props_clean.pop(col)\n",
    "    \n",
    "# drop nans\n",
    "soil_survey_props_clean.dropna(inplace=True)\n",
    "soil_survey_props_clean.reset_index(drop=True, inplace=True)\n",
    "soil_survey_props_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new soil_color, keeping on those that are kept here and re-indexing to ATS indices\n",
    "soil_color_new = -np.ones_like(soil_color)\n",
    "for new_id, mukey in enumerate(soil_survey_props_clean['native_index']):\n",
    "    soil_color_new[np.where(soil_color == mukey)] = 1000+new_id\n",
    "    \n",
    "# image the new soil_color\n",
    "indices, cmap, norm, ticks, labels = workflow.colors.generate_indexed_colormap(soil_color_new, cmap='tab20c')\n",
    "fig, ax = workflow.plot.get_ax(crs)\n",
    "\n",
    "mp = workflow.plot.mesh(m2, crs, ax=ax, facecolor='color',\n",
    "                        linewidth=0, color=soil_color_new, \n",
    "                        cmap=cmap, norm=norm)\n",
    "\n",
    "workflow.colors.colorbar_index(ncolors=len(np.unique(soil_color_new)), cmap=cmap, labels=labels) \n",
    "\n",
    "ax.set_title('soil type index')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLYHMPS geologic layer\n",
    "\n",
    "GLYHMPS is complete in that it does not appear to have missing data, but does not have texture properties needed for Water Retention Models.  Instead we rely on scaling laws to fill the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the GLYHMPS geologic structure data as shapes and project it onto the mesh\n",
    "target_bounds = watershed.exterior().bounds\n",
    "logging.info('target bounds: {}'.format(target_bounds))\n",
    "\n",
    "_, geo_survey, geo_survey_props = workflow.get_shapes(sources['geologic structure'], \n",
    "                                                      target_bounds, crs, crs, properties=True)\n",
    "\n",
    "# -- log the bounds targeted and found\n",
    "logging.info('shape union bounds: {}'.format(\n",
    "    shapely.ops.cascaded_union(geo_survey).bounds))\n",
    "\n",
    "# -- determine the ID for each soil unit; this uniquely identifies formation\n",
    "#    properties\n",
    "geo_ids = np.array([shp.properties['id'] for shp in geo_survey], np.int32)\n",
    "\n",
    "# -- color a raster by the polygons (this makes identifying a triangle's value much \n",
    "#    more efficient)\n",
    "geo_color_raster, geo_color_profile, img_bounds = \\\n",
    "            workflow.color_raster_from_shapes(target_bounds, 10, geo_survey,\n",
    "                                              geo_ids, crs)\n",
    "\n",
    "# -- resample the raster to the triangles\n",
    "geo_color = workflow.values_from_raster(m2.centroids(), crs, \n",
    "                                         geo_color_raster, geo_color_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the properties that appear in the mesh\n",
    "geo_survey_props.set_index('id', inplace=True, drop=False)\n",
    "geo_survey_props = geo_survey_props.loc[np.unique(geo_color), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the geologic formation id\n",
    "indices, cmap, norm, ticks, labels = workflow.colors.generate_indexed_colormap(geo_color, cmap='tab20b')\n",
    "\n",
    "fig, ax = workflow.plot.get_ax(crs)\n",
    "\n",
    "mp = workflow.plot.mesh(m2, crs, ax=ax, facecolor='color',\n",
    "                        linewidth=0, color=geo_color, \n",
    "                        cmap=cmap, norm=norm)\n",
    "\n",
    "workflow.colors.colorbar_index(ncolors=len(np.unique(geo_color)), cmap=cmap, labels=labels) \n",
    "\n",
    "ax.set_title('geol type index')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot permeability of the underlying geologic layer\n",
    "iprop = np.empty(geo_color.shape, 'd')\n",
    "for i in geo_survey_props.index:\n",
    "    iprop[geo_color == i] = geo_survey_props.loc[i, 'permeability [m^2]']\n",
    "    \n",
    "fig, ax = workflow.plot.get_ax(crs)\n",
    "mp = workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color=np.log10(iprop), edgecolor='gray', cmap='jet')\n",
    "cbar = fig.colorbar(mp, shrink=0.5)\n",
    "ax.set_title('geology log permeability [m^2]')\n",
    "ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot porosity of the geologic layer\n",
    "iprop = np.empty(geo_color.shape, 'd')\n",
    "for i in geo_survey_props.index:\n",
    "    iprop[geo_color == i] = geo_survey_props.loc[i, 'porosity [-]']\n",
    "\n",
    "fig, ax = workflow.plot.get_ax(crs)\n",
    "mp = workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color=iprop, edgecolor='gray', cmap='jet')\n",
    "cbar = fig.colorbar(mp, shrink=0.5)\n",
    "ax.set_title('geology porosity [-]')\n",
    "ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note there are clearly some common regions -- no need to duplicate those with identical values.\n",
    "geo_survey_props_clean = geo_survey_props.copy()\n",
    "geo_survey_props_clean.pop('logk_stdev [-]')\n",
    "geo_survey_props_clean.rename(columns={'id':'native_index'}, inplace=True)\n",
    "\n",
    "\n",
    "def reindex_remove_duplicates(df, index=None):\n",
    "    \"\"\"Removes duplicates, creating a new index and saving the old index as tuples of duplicate values. In place!\"\"\"\n",
    "    if index is not None:\n",
    "        if index in df:\n",
    "            df.set_index(index, drop=True, inplace=True)\n",
    "    \n",
    "    index_name = df.index.name\n",
    "\n",
    "    # identify duplicate rows\n",
    "    duplicates = list(df.groupby(list(df)).apply(lambda x: tuple(x.index)))\n",
    "\n",
    "    # order is preserved\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df[index_name] = duplicates\n",
    "    return\n",
    "\n",
    "reindex_remove_duplicates(geo_survey_props_clean, 'native_index')\n",
    "geo_survey_props_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new geologic layer color, keeping on those that are kept here and re-indexing to ATS indices\n",
    "geo_color_new = -np.ones_like(geo_color)\n",
    "for new_id, old_id_dups in enumerate(geo_survey_props_clean['native_index']):\n",
    "    for old_id in old_id_dups:\n",
    "        geo_color_new[np.where(geo_color == old_id)] = 100+new_id\n",
    "    \n",
    "# image the new geo_color\n",
    "indices, cmap, norm, ticks, labels = workflow.colors.generate_indexed_colormap(geo_color_new, cmap='tab20c')\n",
    "fig, ax = workflow.plot.get_ax(crs)\n",
    "\n",
    "mp = workflow.plot.mesh(m2, crs, ax=ax, facecolor='color',\n",
    "                        linewidth=0, color=geo_color_new, \n",
    "                        cmap=cmap, norm=norm)\n",
    "\n",
    "workflow.colors.colorbar_index(ncolors=len(np.unique(geo_color_new)), cmap=cmap, labels=labels) \n",
    "\n",
    "ax.set_title('geologic type index')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth-to-bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth to bedrock is taken from the [SoilGrids](http://globalchange.bnu.edu.cn/research/dtb.jsp) product.  Here we download a US-based, clipped version of this global product, as file sizes are quite large (all products potentially used total over 100GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTB_source = workflow.source_list.structure_sources['SoilGrids2017']\n",
    "DTB_profile, DTB_raster = workflow.get_raster_on_shape(DTB_source, watershed.exterior(), crs, \n",
    "                                                       nodata=-99999, variable='BDTICM')\n",
    "\n",
    "# resample the raster to the triangles\n",
    "DTB_raster = DTB_raster/100 #convert from cm to m\n",
    "DTB = workflow.values_from_raster(m2.centroids(), crs, DTB_raster, DTB_profile, algorithm='piecewise bilinear')\n",
    "DTB = np.where(DTB >= 0, DTB, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the resulting surface mesh\n",
    "fig, ax = workflow.plot.get_ax(crs, window=[0.05,0.1,0.9,0.8])\n",
    "cbax = fig.add_axes([.95,0.1,0.05,0.8])\n",
    "\n",
    "mp = workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color=DTB, cmap='plasma_r', edgecolor='white', linewidth=0.1)\n",
    "cbar = fig.colorbar(mp, orientation=\"vertical\", cax=cbax)\n",
    "workflow.plot.hucs(watershed, crs, ax=ax, color='k', linewidth=1)\n",
    "\n",
    "ax.set_aspect('equal', 'datalim')\n",
    "ax.axis('off')\n",
    "\n",
    "cbar.ax.set_title('DTB [m]')\n",
    "fig.savefig(get_filename('coweeta_dtb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A combined, complete product?\n",
    "\n",
    "As a default, we would like a material-driven (e.g. not fields for porosity, perm, etc, but soil classes, each with a common porosity/permeability/vG curve) default that is valid everywhere.  That makes it clear that we must rely on GLHYMPS as the only material-based product that is valid everywhere.  Other products may be layered on top of this, replacing GLHYMPS values, but the underlying layer should be based on GLHYMPS.  To fill in the van Genuchten properties, we relate alpha to permeability and choose a single common n and s_r.\n",
    "\n",
    "Where available, we then choose to use SSURGO as a layer on top of GLHYMPS.  So start by using all GLHYMPS values, then override ones where SSURGO is valid with those values.  This will be the second model, and has then three layers -- a bedrock layer, a soil layer from 0 to 2m, and a geologic layer, using GLHYMPS values.  SoilGrids depth-to-bedrock will be used to provide the transition between bedrock and (where > 2m) the GLHYMPS \"geologic\" layer or (where < 2m) the SSURGO \"soil\" layer.  Where SSURGO has no values, the underlying GLHYMPS values will be used even in the top 2m.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Mesh extrusion\n",
    "\n",
    "Given the surface mesh and material IDs on both the surface and subsurface, we can extrude the surface mesh in the vertical to make a 3D mesh.\n",
    "\n",
    "First, all integer IDs in Exodus files must be unique.  This includes Material IDs, side sets, etc.  We create the Material ID map and data frame.  This is used to standardize IDs from multiple data sources.  Traditionally, ATS numbers Material IDs/Side Sets as:\n",
    "\n",
    "* 0-9 : reserved for boundaries, surface/bottom, etc\n",
    "* 10-99 : Land Cover side sets, typically NLCD IDs are used\n",
    "* 100-999 : geologic layer material IDs. 999 is reserved for bedrock.\n",
    "* 1000-9999 : soil layer material IDs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map SSURGO mukey to ATS_ID\n",
    "soil_survey_props_clean['ats_id'] = range(1000, 1000+len(soil_survey_props_clean))\n",
    "soil_survey_props_clean.set_index('ats_id', inplace=True)\n",
    "\n",
    "# map GLHYMPS id to ATS_ID\n",
    "geo_survey_props_clean['ats_id'] = range(100, 100+len(geo_survey_props_clean))\n",
    "geo_survey_props_clean.set_index('ats_id', inplace=True)\n",
    "\n",
    "bedrock_props = workflow.soil_properties.get_bedrock_properties()\n",
    "\n",
    "# merge the properties databases\n",
    "subsurface_props = pandas.concat([geo_survey_props_clean,\n",
    "                                  soil_survey_props_clean,\n",
    "                                  bedrock_props])\n",
    "\n",
    "# save the properties to disk for use in generating input file\n",
    "subsurface_props.to_csv(get_filename('Coweeta_subsurface_properties.csv'))\n",
    "subsurface_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extrude the DEM to create a 3D mesh.\n",
    "\n",
    "The most difficult aspect of extrusion is creating meshes that:\n",
    "1. aren't huge numbers of cells\n",
    "2. aren't huge cell thicknesses, especially near the surface\n",
    "3. follow implied interfaces, e.g. bottom of soil and bottom of geologic layer\n",
    "\n",
    "This is an iterative process that requires some care and some art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we choose the bottom of the domain to be bigger than the maximum of the depth to bedrock.  \n",
    "# This is really up to the user...\n",
    "total_thickness = np.ceil(DTB.max())\n",
    "print(f'total thickness: {total_thickness} m')\n",
    "\n",
    "total_thickness = 41.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dz structure for the top 2m of soil\n",
    "#\n",
    "# here we try for 10 cells, starting at 5cm at the top and going to 50cm at the bottom of the 2m thick soil\n",
    "dzs, res = workflow.mesh.optimize_dzs(0.05, 0.5, 2, 10)\n",
    "print(dzs)\n",
    "print(sum(dzs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks like it would work out, with rounder numbers:\n",
    "dzs_soil = [0.05, 0.05, 0.05, 0.12, 0.23, 0.5, 0.5, 0.5]\n",
    "print(sum(dzs_soil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 41m total thickness, minus 2m soil thickness, leaves us with 39 meters to make up.\n",
    "# optimize again...\n",
    "dzs2, res2 = workflow.mesh.optimize_dzs(1, 10, 39, 8)\n",
    "print(dzs2)\n",
    "print(sum(dzs2))\n",
    "\n",
    "# how about...\n",
    "dzs_geo = [1.0, 3.0,] + 7*[5.0,]\n",
    "print(dzs_geo)\n",
    "print(sum(dzs_geo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer extrusion\n",
    "# -- data structures needed for extrusion\n",
    "layer_types = []\n",
    "layer_data = []\n",
    "layer_ncells = []\n",
    "layer_mat_ids = []\n",
    "\n",
    "# -- soil layer --\n",
    "depth = 0\n",
    "for dz in dzs_soil:\n",
    "    depth += 0.5 * dz\n",
    "    layer_types.append('constant')\n",
    "    layer_data.append(dz)\n",
    "    layer_ncells.append(1)\n",
    "    \n",
    "    br_or_geo = np.where(depth < DTB, geo_color_new, 999)\n",
    "    soil_or_br_or_geo = np.where(np.bitwise_and(soil_color_new > 0, depth < soil_thickness),\n",
    "                                 soil_color_new,\n",
    "                                 br_or_geo)\n",
    "    \n",
    "    layer_mat_ids.append(soil_or_br_or_geo)\n",
    "    depth += 0.5 * dz\n",
    "    \n",
    "# -- geologic layer --\n",
    "for dz in dzs_geo:\n",
    "    depth += 0.5 * dz\n",
    "    layer_types.append('constant')\n",
    "    layer_data.append(dz)\n",
    "    layer_ncells.append(1)\n",
    "    layer_mat_ids.append(np.where(depth < DTB, geo_color_new, 999))\n",
    "    depth += 0.5 * dz\n",
    "\n",
    "# print the summary\n",
    "workflow.mesh.Mesh3D.summarize_extrusion(layer_types, layer_data, \n",
    "                                            layer_ncells, layer_mat_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrude\n",
    "m3 = workflow.mesh.Mesh3D.extruded_Mesh2D(m2, layer_types, layer_data, \n",
    "                                             layer_ncells, layer_mat_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back on land cover side sets\n",
    "surf_ss = m3.side_sets[1]\n",
    "\n",
    "for index, name in zip(nlcd_indices, nlcd_labels):\n",
    "    where = np.where(nlcd_color_new == index)[0]\n",
    "    print(index, name, len(where))\n",
    "    ss = workflow.mesh.SideSet(name, int(index), \n",
    "                            [surf_ss.elem_list[w] for w in where],\n",
    "                            [surf_ss.side_list[w] for w in where])        \n",
    "    m3.side_sets.append(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "try:\n",
    "    os.remove(get_filename('Coweeta.exo'))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "m3.write_exodus(get_filename('Coweeta.exo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect the DayMet raster covering this area\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import workflow.daymet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"1-2019\"\n",
    "end = \"365-2020\"\n",
    "bounds = watershed.exterior().bounds\n",
    "\n",
    "dat, x, y = workflow.daymet.collectDaymet(bounds, crs, start, end)\n",
    "ats = workflow.daymet.daymetToATS(dat)\n",
    "attrs = workflow.daymet.getAttrs(bounds, start, end)\n",
    "daymet_filename = get_filename('Coweeta_DayMet_2019_2020.h5')\n",
    "workflow.daymet.writeHDF5(ats, x, y, attrs, daymet_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now write the ATS input file for all of this info..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an ATS \"main\" input spec list\n",
    "main_list = ats_input_spec.public.get_main()\n",
    "\n",
    "# this is up to the user -- where will the newly created mesh be \n",
    "# put relative to the simulation directory?  Note this could be \n",
    "# different than the filename specified above where the mesh was\n",
    "# saved, as the run might be in a different location than this \n",
    "# script.\n",
    "mesh_filename = get_filename('Coweeta.exo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the subsurface and surface domains\n",
    "#\n",
    "# Note this also adds a \"computational domain\" region to the region list, and a vis spec \n",
    "# for \"domain\"\n",
    "ats_input_spec.public.add_domain(main_list, \n",
    "                                 domain_name='domain', \n",
    "                                 dimension=3, \n",
    "                                 mesh_type='read mesh file', \n",
    "                                 mesh_args={'file':mesh_filename})\n",
    "main_list['mesh']['domain']['build columns from set'] = 'surface'\n",
    "\n",
    "# Note this also adds a \"surface domain\" region to the region list and a vis spec for \n",
    "# \"surface\"\n",
    "ats_input_spec.public.add_domain(main_list,\n",
    "                                domain_name='surface',\n",
    "                                dimension=2,\n",
    "                                mesh_type='surface',\n",
    "                                mesh_args={'surface sideset name':'surface'})\n",
    "\n",
    "# Add the snow and canopy domains, which are aliases to the surface\n",
    "ats_input_spec.public.add_domain(main_list,\n",
    "                                domain_name='snow',\n",
    "                                dimension=2,\n",
    "                                mesh_type='aliased',\n",
    "                                mesh_args={'target':'surface'})\n",
    "ats_input_spec.public.add_domain(main_list,\n",
    "                                domain_name='canopy',\n",
    "                                dimension=2,\n",
    "                                mesh_type='aliased',\n",
    "                                mesh_args={'target':'surface'})\n",
    "\n",
    "# add the \"surface boundary\" list\n",
    "ats_input_spec.public.add_region(main_list,\n",
    "                                region_name='surface boundary',\n",
    "                                region_type='boundary',\n",
    "                                region_args={'entity': 'FACE'})\n",
    "\n",
    "def add_labeled_set(name, label, entity_kind):\n",
    "    \"\"\"Helper function to add labeled sets to the main list\"\"\"\n",
    "    ats_input_spec.public.add_region(main_list,\n",
    "                                    region_name=name,\n",
    "                                    region_type='labeled set',\n",
    "                                    region_args={'label' : str(label), \n",
    "                                                 'file' : mesh_filename,\n",
    "                                                 'format' : 'Exodus II',\n",
    "                                                 'entity' : entity_kind})\n",
    "\n",
    "# add the bottom and surface sets\n",
    "add_labeled_set('bottom_face', 1, 'FACE')\n",
    "add_labeled_set('surface', 2, 'FACE')\n",
    "\n",
    "# add the NLCD labeled sets\n",
    "for index, name in zip(nlcd_indices, nlcd_labels):\n",
    "    add_labeled_set(name, index, 'FACE')\n",
    "\n",
    "# add soil sets: note we need a way to name the set, so we use, e.g. SSURGO-MUKEY.\n",
    "def soil_set_name(ats_id):\n",
    "    if ats_id == 999:\n",
    "        return 'bedrock'\n",
    "    source = subsurface_props.loc[ats_id]['source']\n",
    "    native_id = subsurface_props.loc[ats_id]['native_index']\n",
    "    if type(native_id) in [tuple,list]:\n",
    "        native_id = native_id[0]\n",
    "    return f\"{source}-{native_id}\"\n",
    "\n",
    "for ats_id in subsurface_props.index:\n",
    "    set_name = soil_set_name(ats_id)\n",
    "    add_labeled_set(set_name, ats_id, 'CELL')\n",
    "\n",
    "print(main_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next write a land-cover section for each NLCD type\n",
    "land_cover_list = ats_input_spec.public.known_specs['land-cover-spec-list']\n",
    "main_list['state']['initial conditions']['land cover types'] = land_cover_list\n",
    "\n",
    "def add_land_cover_default(main, name):\n",
    "    \"\"\"Adds a default land-cover type to the spec.\"\"\"\n",
    "    lc = main['state']['initial conditions']['land cover types'].append_empty(name)\n",
    "    \n",
    "    # set some basic defaults\n",
    "    lc['Priestley-Taylor alpha of snow [-]'] = 1.26\n",
    "    lc['Priestley-Taylor alpha of bare ground [-]'] = 1.26\n",
    "    lc['Priestley-Taylor alpha of canopy [-]'] = 1.26\n",
    "    lc['Priestley-Taylor alpha of transpiration [-]'] = 0.63\n",
    "\n",
    "    lc['albedo of bare ground [-]'] = 0.4\n",
    "    lc['emissivity of bare ground [-]'] = 0.98\n",
    "    lc['albedo of canopy [-]'] = 0.11\n",
    "    lc['emissivity of canopy [-]'] = 0.95\n",
    "    \n",
    "    lc[\"Beer's law extinction coefficient, shortwave [-]\"] = 0.6\n",
    "    lc[\"Beer's law extinction coefficient, longwave [-]\"] = 5\n",
    "    \n",
    "    lc[\"snow transition depth [m]\"] = 0.02\n",
    "    lc[\"dessicated zone thickness [m]\"] = 0.1\n",
    "    lc[\"Clapp and Hornberger b [-]\"] = 1\n",
    "    \n",
    "    # defaults for grass/no vei\n",
    "    lc['rooting depth max [m]'] = 5.\n",
    "    lc['rooting profile alpha [-]'] = 11.0\n",
    "    lc['rooting profile beta [-]'] = 2.0\n",
    "    \n",
    "    \n",
    "    # Note, the mafic potential values are likely pretty bad for the types of van Genuchten \n",
    "    # curves we are using (https://www.sciencedirect.com/science/article/pii/S0168192314000483).\n",
    "    # Likely they need to be modified.  Note that these values are in [mm] from CLM TN 4.5 table 8.1, so the \n",
    "    # factor of 10 converts to [Pa].\n",
    "    #\n",
    "    # instead of using a factor of 10, we use a factor of 1 for closed and .1 for open to make this more \n",
    "    # physically viable for our VG models\n",
    "    lc['mafic potential at fully closed stomata [Pa]'] = 275000.\n",
    "    lc['mafic potential at fully open stomata [Pa]'] = 74000. * .1\n",
    "    \n",
    "    # by default we let the LAI take care of this rather than turn off deciduous \n",
    "    # transpiration manually the way that PRMS does it. \n",
    "    lc['leaf on time [doy]'] = -1\n",
    "    lc['leaf off time [doy]'] = -1\n",
    "    return lc\n",
    "\n",
    "# add default lists for all types\n",
    "for index, name in zip(nlcd_indices, nlcd_labels):\n",
    "    lc = add_land_cover_default(main_list, name)\n",
    "\n",
    "# update some defaults\n",
    "# ['Other', 'Deciduous Forest', 'Evergreen Forest', 'Shrub/Scrub']\n",
    "# note, these are from the CLM Technical Note v4.5\n",
    "#\n",
    "# Rooting depth curves from CLM TN 4.5 table 8.3\n",
    "#\n",
    "# Note, the mafic potential values are likely pretty bad for the types of van Genuchten \n",
    "# curves we are using (ETC -- add paper citation about this topic).  Likely they need\n",
    "# to be modified.  Note that these values are in [mm] from CLM TN 4.5 table 8.1, so the \n",
    "# factor of 10 converts to [Pa]\n",
    "#\n",
    "# Note, albedo of canopy taken from CLM TN 4.5 table 3.1\n",
    "land_cover_list['Mixed Forest']['rooting profile alpha [-]'] = 7.0\n",
    "land_cover_list['Mixed Forest']['rooting profile beta [-]'] = 2.0\n",
    "land_cover_list['Mixed Forest']['rooting depth max [m]'] = 10.0\n",
    "land_cover_list['Mixed Forest']['mafic potential at fully closed stomata [Pa]'] = 255000\n",
    "land_cover_list['Mixed Forest']['mafic potential at fully open stomata [Pa]'] = 66000 * .1\n",
    "land_cover_list['Mixed Forest']['albedo of canopy [-]'] = 0.07\n",
    "\n",
    "land_cover_list['Deciduous Forest']['rooting profile alpha [-]'] = 6.0\n",
    "land_cover_list['Deciduous Forest']['rooting profile beta [-]'] = 2.0\n",
    "land_cover_list['Deciduous Forest']['rooting depth max [m]'] = 10.0\n",
    "land_cover_list['Deciduous Forest']['mafic potential at fully closed stomata [Pa]'] = 224000\n",
    "land_cover_list['Deciduous Forest']['mafic potential at fully open stomata [Pa]'] = 35000 * .10\n",
    "land_cover_list['Deciduous Forest']['albedo of canopy [-]'] = 0.1\n",
    "\n",
    "print(land_cover_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we write soil property specs for porosity, permeability, and WRMs\n",
    "poro_eval = main_list['state']['field evaluators'].append_empty('base_porosity')\n",
    "poro_eval.set_type('independent variable', ats_input_spec.public.known_specs['independent-variable-evaluator-spec'])\n",
    "poro_eval['constant in time'] = True\n",
    "\n",
    "perm_eval = main_list['state']['field evaluators'].append_empty('permeability')\n",
    "perm_eval.set_type('independent variable', ats_input_spec.public.known_specs['independent-variable-evaluator-spec'])\n",
    "perm_eval['constant in time'] = True\n",
    "\n",
    "wrm_eval = main_list['state']['field evaluators'].append_empty('saturation_liquid')\n",
    "wrm_eval.set_type('WRM rel perm', ats_input_spec.public.known_specs['rel-perm-evaluator-spec'])\n",
    "wrm_eval['boundary rel perm strategy'] = 'surface rel perm'\n",
    "wrm_eval['permeability rescaling'] = 1e7\n",
    "\n",
    "\n",
    "print(main_list['state']['field evaluators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_region_set_value(evaluator, ats_id, val):\n",
    "    region = soil_set_name(ats_id)\n",
    "    my_func = evaluator['function'].append_empty(region)\n",
    "    my_func['region'] = region\n",
    "    my_func['component'] = 'cell'\n",
    "    my_func_const = my_func['function'].set_type('constant', \n",
    "                                                 ats_input_spec.public.known_specs['function-constant-spec'])\n",
    "    my_func_const['value'] = float(val)\n",
    "\n",
    "def add_wrm(evaluator, ats_id, props):\n",
    "    region = soil_set_name(ats_id)\n",
    "    wrm = wrm_eval['WRM parameters'].append_empty(region)\n",
    "    wrm.set_type('van Genuchten', ats_input_spec.public.known_specs['WRM-van-Genuchten-spec'])\n",
    "\n",
    "    wrm['region'] = region\n",
    "    wrm['van Genuchten alpha [Pa^-1]'] = float(props['van Genuchten alpha [Pa^-1]'])\n",
    "    wrm['van Genuchten n [-]'] = float(props['van Genuchten n [-]'])\n",
    "    wrm['residual saturation [-]'] = float(props['residual saturation [-]'])\n",
    "    wrm['smoothing interval width [saturation]'] = 0.05\n",
    "    \n",
    "    \n",
    "for ats_id in subsurface_props.index:\n",
    "    add_region_set_value(poro_eval, ats_id, subsurface_props.loc[ats_id]['porosity [-]'])\n",
    "    add_region_set_value(perm_eval, ats_id, subsurface_props.loc[ats_id]['permeability [m^2]'])\n",
    "    add_wrm(wrm_eval, ats_id, subsurface_props.loc[ats_id])\n",
    "\n",
    "    \n",
    "print(main_list['state']['field evaluators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this file is not a complete ATS run, but it can be used to copy/paste into other xml files...\n",
    "ats_input_spec.io.write(main_list, get_filename('ATS_basics.xml'))\n",
    "main_xml = ats_input_spec.io.to_xml(main_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's add these to an existing demo problem\n",
    "xml = aio.fromFile('ATS_template.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and replace the mesh list\n",
    "#\n",
    "# Note, this should be migrated into python \"backend\" library for watershed workflow\n",
    "mesh_i = next(i for (i,el) in enumerate(xml) if el.get('name') == 'mesh')\n",
    "xml[mesh_i] = asearch.child_by_name(main_xml, 'mesh')\n",
    "\n",
    "# find and replace the regions list\n",
    "region_i = next(i for (i,el) in enumerate(xml) if el.get('name') == 'regions')\n",
    "xml[region_i] = asearch.child_by_name(main_xml, 'regions')\n",
    "\n",
    "# find and replace the WRMs list -- note here we only replace the inner \"WRM parameters\" because the\n",
    "# demo has this in the PK, not in the field evaluators list\n",
    "wrm_list = asearch.find_path(xml, ['PKs', 'water retention evaluator'])\n",
    "wrm_i = next(i for (i,el) in enumerate(wrm_list) if el.get('name') == 'WRM parameters')\n",
    "wrm_list[wrm_i] = asearch.find_path(main_xml, ['state','field evaluators','WRM parameters'])\n",
    "\n",
    "# find and replace porosity, permeability\n",
    "fe_list = asearch.find_path(xml, ['state', 'field evaluators'])\n",
    "poro_i = next(i for (i,el) in enumerate(fe_list) if el.get('name') == 'base_porosity')\n",
    "fe_list[poro_i] = asearch.find_path(main_xml, ['state', 'field evaluators', 'base_porosity'])\n",
    "\n",
    "perm_i = next(i for (i,el) in enumerate(fe_list) if el.get('name') == 'permeability')\n",
    "fe_list[perm_i] = asearch.find_path(main_xml, ['state', 'field evaluators', 'permeability'])\n",
    "\n",
    "# find and replace land cover\n",
    "consts_list = asearch.find_path(xml, ['state', 'initial conditions'])\n",
    "lc_i = next(i for (i,el) in enumerate(consts_list) if el.get('name') == 'land cover types')\n",
    "consts_list[lc_i] = asearch.find_path(main_xml, ['state', 'initial conditions', 'land cover types'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the DayMet filenames\n",
    "for var in ['surface-incoming_shortwave_radiation',\n",
    "            'surface-precipitation_rain',\n",
    "            'snow-precipitation',\n",
    "            'surface-air_temperature',\n",
    "            'surface-relative_humidity',\n",
    "            'surface-temperature',\n",
    "            'canopy-temperature']:\n",
    "    try:\n",
    "        par = asearch.find_path(xml, ['state', 'field evaluators', var, 'file'])\n",
    "    except aerrors.MissingXMLError:\n",
    "        pass\n",
    "    else:\n",
    "        par.set('value', daymet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the updated file -- this is a complete, usable ATS input file.\n",
    "aio.toFile(xml, '../Coweeta.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a giant plot of all the WRMs used in this simulation\n",
    "import plot_wrm # $ATS_SRC_DIR/tools/utils\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cm_gl = workflow.colors.cm_mapper(100, 103, 'winter')\n",
    "cm_ss = workflow.colors.cm_mapper(1000,1017, 'autumn')\n",
    "\n",
    "for i in subsurface_props.index:\n",
    "    if i < 999:\n",
    "        cl = cm_gl(i)\n",
    "    else:\n",
    "        cl = cm_ss(i)\n",
    "    alpha = subsurface_props.loc[i]['van Genuchten alpha [Pa^-1]']\n",
    "    n = subsurface_props.loc[i]['van Genuchten n [-]']\n",
    "    sr = subsurface_props.loc[i]['residual saturation [-]']\n",
    "    vg = plot_wrm.VanGenuchten(alpha, n, sr)\n",
    "    plot_wrm.plot(vg, ax, cl)\n",
    "    \n",
    "\n",
    "wp1 = plot_wrm.WiltingPointLimiter(7500, 275000)\n",
    "wp2 = plot_wrm.WiltingPointLimiter(6600, 255000)\n",
    "wp3 = plot_wrm.WiltingPointLimiter(3500, 224000)\n",
    "wp4 = plot_wrm.WiltingPointLimiter(343245, 2196768)\n",
    "for wp in [wp1, wp2, wp3, wp4]:\n",
    "    plot_wrm.plot(wp, ax, 'k')\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow is now complete.  To run the resulting file (noting that this is not a very \n",
    "useful simulation thanks to the short duration and lack of spinup):\n",
    "\n",
    "    cd ../\n",
    "    mkdir run\n",
    "    cd run\n",
    "    mpiexec -n 32 ats ../Coweeta.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:watershed_workflow_20210603] *",
   "language": "python",
   "name": "conda-env-watershed_workflow_20210603-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
